---
title: "Final project; Data processing - Data Science for Linguists (LING 2340)
Fall 2022"
subtitle: "Data Processing - Progress report 3"
author: "Gianina Morales"
date: "12/1/2022"
output:
  github_document:
    toc: TRUE
---
# Data processing (NEW CONTINUING - Part 2) - Progress report 3

---

```{r setup, message=FALSE, warning=FALSE}
library(tidytext)
library(tidyverse)
library(tm)
library(stopwords)
library(stringi)
library(topicmodels)
```

## Topic modeling

**Latent Dirichlet allocation (LDA)** a mathematical method for finding topic probabilities in a corpus. The mechanism includes two elements:

* LDA allows finding topics in a series of documents automatically. The number of topics responds to a parameter set by the researcher (*n*=x). The topics listed are weighted by their relative importance in the corpus, informed by the frequency and distribution of words.

* Each topic includes a series of words that the algorithm *estimates* is part of the topic. The automatic process implies probabilities of word-topic association. That is why some words appear in different topics.

I will apply the method following the book [Text Mining with R](https://www.tidytextmining.com/index.html), particularly the chapters six ("Topic modeling") and nine ("case study").

* **number of topics**

The processing and analysis of a large corpus (such as mine, with more than 9 million words) take considerable time. After looking at other studies and considering the scale of my project, I decided to set the number of topics at 10. Also, I have discovered that my computer does not have the memory to run the topic model with the total corpus (It shut down) This way, I will analyze ten topics by decade to make the trend comparison. 

1. Application to the corpus of 2020 to 2022

1.1. Topic model for ten topics

```{r}
#Loading Rds
tidy_corpus20_22 <- read_rds("Private/tidy_corpus_all/tidy_corpus20_22.Rds")

#creating document-term matrix (necessary to apply the package)
corpus20_22_dtm <- 
  tidy_corpus20_22 %>% 
  count(word, id) %>%
    cast_dtm(id, word, n)

corpus20_22_dtm

# creating a model with 10 topics
corpus20_22_topicmodel <- 
LDA(corpus20_22_dtm, k = 10, control = list(seed = 1234))

corpus20_22_topicmodel
```
The result is a model that I will use as a base in the next steps

1.2. Word-topic probability

```{r}
# per topic per word probabilities
corpus20_22_topics <- tidy(corpus20_22_topicmodel, matrix = "beta")
corpus20_22_topics
```

*Process*

Most common words inside each topic

```{r}
#finding the terms to analyze the topics
corpus20_22_terms <- corpus20_22_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 25) %>% 
  ungroup() %>%
  arrange(topic, -beta)

#visualization of ten most common words by topic
corpus20_22_10terms <- corpus20_22_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)
#Chart
corpus20_22_10terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```

*Analysis*

* see note at the end

2. Application to the corpus of 2010 to 2019

2.1. Topic model for ten topics

```{r}
#Loading Rds
tidy_corpus10_19 <- read_rds("Private/tidy_corpus_all/tidy_corpus10_19.Rds")

#creating document-term matrix (necessary to apply the package)
corpus10_19_dtm <- 
  tidy_corpus10_19 %>% 
  count(word, id) %>%
    cast_dtm(id, word, n)

corpus10_19_dtm

# creating a model with 10 topics
corpus10_19_topicmodel <- 
LDA(corpus10_19_dtm, k = 10, control = list(seed = 1234))

corpus10_19_topicmodel
```
The result is a model that I will use as a base in the next steps

2.2. Word-topic probability

```{r}
# per topic per word probabilities
corpus10_19_topics <- tidy(corpus10_19_topicmodel, matrix = "beta")
corpus10_19_topics
```

*Process*

Most common words inside each topic

```{r}
#finding the terms to analyze the topics
corpus10_19_terms <- corpus10_19_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 25) %>% 
  ungroup() %>%
  arrange(topic, -beta)

#visualization of ten most common words by topic
corpus10_19_10terms <- corpus10_19_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)
#Chart
corpus10_19_10terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```
*Analysis*

* see note at the end

3. Application to the corpus of 2000 to 2009

3.1. Topic model for ten topics

```{r}
#Loading Rds
tidy_corpus00_09 <- read_rds("Private/tidy_corpus_all/tidy_corpus00_09.Rds")

#creating document-term matrix (necessary to apply the package)
corpus00_09_dtm <- 
  tidy_corpus00_09 %>% 
  count(word, id) %>%
    cast_dtm(id, word, n)

corpus00_09_dtm

# creating a model with 10 topics
corpus00_09_topicmodel <- 
LDA(corpus00_09_dtm, k = 10, control = list(seed = 1234))

corpus00_09_topicmodel
```
The result is a model that I will use as a base in the next steps

3.2. Word-topic probability

```{r}
# per topic per word probabilities
corpus00_09_topics <- tidy(corpus00_09_topicmodel, matrix = "beta")
corpus00_09_topics
```

*Process*

Most common words inside each topic

```{r}
#finding the terms to analyze the topics
corpus00_09_terms <- corpus00_09_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 25) %>% 
  ungroup() %>%
  arrange(topic, -beta)

#visualization of ten most common words by topic
corpus00_09_10terms <- corpus00_09_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)
#Chart
corpus00_09_10terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```
*Analysis*

* see note at the end

4. Application to the corpus of 1990 to 1999

4.1. Topic model for ten topics

```{r}
#Loading Rds
tidy_corpus90_99 <- read_rds("Private/tidy_corpus_all/tidy_corpus90_99.Rds")

#creating document-term matrix (necessary to apply the package)
corpus90_99_dtm <- 
  tidy_corpus90_99 %>% 
  count(word, id) %>%
    cast_dtm(id, word, n)

corpus90_99_dtm

# creating a model with 10 topics
corpus90_99_topicmodel <- 
LDA(corpus90_99_dtm, k = 10, control = list(seed = 1234))

corpus90_99_topicmodel
```
The result is a model that I will use as a base in the next steps

4.2. Word-topic probability

```{r}
# per topic per word probabilities
corpus90_99_topics <- tidy(corpus90_99_topicmodel, matrix = "beta")
corpus90_99_topics
```

*Process*

Most common words inside each topic

```{r}
#finding the terms to analyze the topics
corpus90_99_terms <- corpus90_99_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 25) %>% 
  ungroup() %>%
  arrange(topic, -beta)

#visualization of ten most common words by topic
corpus90_99_10terms <- corpus90_99_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)
#Chart
corpus90_99_10terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```

*Analysis*

* see note at the end

5. Application to the corpus of 1980 to 1989

5.1. Topic model for ten topics

```{r}
#Loading Rds
tidy_corpus80_89 <- read_rds("Private/tidy_corpus_all/tidy_corpus80_89.Rds")

#creating document-term matrix (necessary to apply the package)
corpus80_89_dtm <- 
  tidy_corpus80_89 %>% 
  count(word, id) %>%
    cast_dtm(id, word, n)

corpus80_89_dtm

# creating a model with 10 topics
corpus80_89_topicmodel <- 
LDA(corpus80_89_dtm, k = 10, control = list(seed = 1234))

corpus80_89_topicmodel
```
The result is a model that I will use as a base in the next steps

5.2. Word-topic probability

```{r}
# per topic per word probabilities
corpus80_89_topics <- tidy(corpus80_89_topicmodel, matrix = "beta")
corpus80_89_topics
```

*Process*

Most common words inside each topic

```{r}
#finding the terms to analyze the topics
corpus80_89_terms <- corpus80_89_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 25) %>% 
  ungroup() %>%
  arrange(topic, -beta)

#visualization of ten most common words by topic
corpus80_89_10terms <- corpus80_89_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)
#Chart
corpus80_89_10terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```

*Analysis*

* see note at the end

6. Application to the corpus of 1969 to 1979

6.1. Topic model for ten topics

```{r}
#Loading Rds
tidy_corpus69_79 <- read_rds("Private/tidy_corpus_all/tidy_corpus69_79.Rds")

#creating document-term matrix (necessary to apply the package)
corpus69_79_dtm <- 
  tidy_corpus69_79 %>% 
  count(word, id) %>%
    cast_dtm(id, word, n)

corpus69_79_dtm

# creating a model with 10 topics
corpus69_79_topicmodel <- 
LDA(corpus69_79_dtm, k = 10, control = list(seed = 1234))

corpus69_79_topicmodel
```
The result is a model that I will use as a base in the next steps

6.2. Word-topic probability

```{r}
# per topic per word probabilities
corpus69_79_topics <- tidy(corpus69_79_topicmodel, matrix = "beta")
corpus69_79_topics
```

*Process*

Most common words inside each topic

```{r}
#finding the terms to analyze the topics
corpus69_79_terms <- corpus69_79_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 25) %>% 
  ungroup() %>%
  arrange(topic, -beta)

#visualization of ten most common words by topic
corpus69_79_10terms <- corpus69_79_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)
#Chart
corpus69_79_10terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```

*Analysis*

* see note at the end

---

**Plan for the analysis**

This time, I had many problems with the data, particularly with my computer capacity (basically, it cannot run the total model). For that reason, I could not have time to perform the analysis as much as I wanted. I plan to finish the analysis in time for the presentation. I will analyze every topic (I have ten by decade with 25 words for each one) to infer a *theme* that name the topic. I will use my literature review as an aid in that process. That will allows me to answer the first question.
Then, I will compare the main topics by decade (I am still trying to figure out how many), looking at correlations (reference, chapter 9.2.1 from [Text Mining with R](https://www.tidytextmining.com/index.html)). That will allow me to see trends and answer the second research question.


#Summary of future actions (pending steps)

1. Analyze each decade and compare their results (trend comparison).

2. Create the final report of my results.

3. Discuss the results in relation to my research questions:

* What are the trends in topics of literacy education research and scholarship over more than five decades (1969-2021) of the focal journals?
* How do the topics have changed over time?

4. Run the whole process with an small sample of open access articles.






```{r}
sessionInfo()
```

