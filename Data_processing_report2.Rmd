---
title: "Final project; Data processing - Data Science for Linguists (LING 2340)
Fall 2022"
subtitle: "Data Processing - Progress report 2"
author: "Gianina Morales"
date: "11/15/2022"
output:
  github_document:
    toc: TRUE
---
# Data processing (new replacement file) - Progress report 2

---

## Pre-processing: Creating and tidying dataframes from the corpus

```{r setup, message=FALSE, warning=FALSE}
library(tidytext)
library(tidyverse)
library(tm)
library(tidyr)
library(stopwords)
library(stringr)
library(stringi)
library(topicmodels)
```

### Turning raw data on raw data frames

***Note***

*After further discussion with my advisor on the possible results of my project, the R processing of my initial data, and the confidential agreement with the publishers of the "leading journal in the field of Literacy education," I decided to include data from a second journal in my corpus. This time, I am considering the journal that publishes the paper conferences of the annual conference of the same organization in charge of the "leading journal in the field of Literacy education." This movement will have three benefits:* 

* *I can share a sample of my raw data (open-access articles).*
* *I will have a more significant corpus to analyze trends over time in topics and language present in a community of literacy research (the leading association).*
* *I will have more tools to comply with the ethics of avoiding individualization in the data analysis*. 

I divided the data into decades. It was very difficult to manipulate the whole data (almost 3,200 `.txt` files), so I created individual data frames. 

**1. 1969-1979**

*Note: I am considering data from the first year in which the two focal journals published, to facilitate further comparison with previous content analysis studies.*

* Reading the `.txt` files

```{r}
# read all the content from one decade of .txt files into a data frame.
raw_corpus69_79 <- tibble(file = dir("Private/1969-1979", full.names = TRUE))%>%
    mutate(text = map(file, read_lines, skip = 2, skip_empty_rows = TRUE, n_max = Inf,locale = default_locale(), na = character(), num_threads = readr_threads())) %>%
    transmute(id = basename(file), text) %>%
    unnest(text) %>%
#replace punctuation signs and numbers with spaces in column 'text'
    mutate(text = str_replace_all(text, "[[:punct:]]+|[0-9]+", " ")) 
  #replace spaces with underscore and other changes in column id
raw_corpus69_79$id <- stri_replace_all_regex(raw_corpus69_79$id, pattern=c(" ","JLR","[aeiou]"), replacement=c("_","JY_","x"), vectorize=F)

#visualization 
sample_n(raw_corpus69_79, 10) 
```
**2. 1980-1989**

* Reading the `.txt` files

```{r}
# read all the content from one decade of .txt files into a data frame.
raw_corpus80_89 <- tibble(file = dir("Private/1980-1989", full.names = TRUE))%>%
    mutate(text = map(file, read_lines, skip = 2, skip_empty_rows = TRUE, n_max = Inf,locale = default_locale(), na = character(), num_threads = readr_threads())) %>%
    transmute(id = basename(file), text) %>%
    unnest(text) %>%
#replace punctuation signs and numbers with spaces in column 'text'
    mutate(text = str_replace_all(text, "[[:punct:]]+|[0-9]+", " ")) 
  #replace spaces with underscore and other changes in column id
raw_corpus80_89$id <- stri_replace_all_regex(raw_corpus80_89$id, pattern=c(" ","JLR","[aeiou]"), replacement=c("_","JY_","x"), vectorize=F)

#visualization 
sample_n(raw_corpus80_89, 10)
```

**3. 1990-1999**

* Reading the `.txt` files

```{r}
# read all the content from one decade of .txt files into a data frame.
raw_corpus90_99 <- tibble(file = dir("Private/1990-1999", full.names = TRUE))%>%
    mutate(text = map(file, read_lines, skip = 2, skip_empty_rows = TRUE, n_max = Inf,locale = default_locale(), na = character(), num_threads = readr_threads())) %>%
    transmute(id = basename(file), text) %>%
    unnest(text) %>%
#replace punctuation signs and numbers with spaces in column 'text'
    mutate(text = str_replace_all(text, "[[:punct:]]+|[0-9]+", " ")) 
  #replace spaces with underscore and other changes in column id
raw_corpus90_99$id <- stri_replace_all_regex(raw_corpus90_99$id, pattern=c(" ","JLR","[aeiou]"), replacement=c("_","JY_","x"), vectorize=F)

#visualization 
sample_n(raw_corpus90_99, 10)
```

**4. 2000-2009**

* Reading the `.txt` files

```{r}
# read all the content from one decade of .txt files into a data frame.
raw_corpus00_09 <- tibble(file = dir("Private/2000-2009", full.names = TRUE))%>%
    mutate(text = map(file, read_lines, skip = 2, skip_empty_rows = TRUE, n_max = Inf,locale = default_locale(), na = character(), num_threads = readr_threads())) %>%
    transmute(id = basename(file), text) %>%
    unnest(text) %>%
#replace punctuation signs and numbers with spaces in column 'text'
    mutate(text = str_replace_all(text, "[[:punct:]]+|[0-9]+", " ")) 
  #replace spaces with underscore and other changes in column id
raw_corpus00_09$id <- stri_replace_all_regex(raw_corpus00_09$id, pattern=c(" ","JLR","[aeiou]"), replacement=c("_","JY_","x"), vectorize=F) 

#visualization 
sample_n(raw_corpus00_09, 10)
```

**5. 2010-2019**

* Reading the `.txt` files

```{r}
# read all the content from one decade of .txt files into a data frame.
raw_corpus10_19 <- tibble(file = dir("Private/2010-2019", full.names = TRUE))%>%
     mutate(text = map(file, read_lines, skip = 2, skip_empty_rows = TRUE, n_max = Inf,locale = default_locale(), na = character(), num_threads = readr_threads())) %>%
    transmute(id = basename(file), text) %>%
    unnest(text) %>%
#replace punctuation signs and numbers with spaces in column 'text'
    mutate(text = str_replace_all(text, "[[:punct:]]+|[0-9]+", " ")) 
  #replace spaces with underscore and other changes in column id
raw_corpus10_19$id <- stri_replace_all_regex(raw_corpus10_19$id, pattern=c(" ","JLR","[aeiou]"), replacement=c("_","JY_","x"), vectorize=F)

#visualization
sample_n(raw_corpus10_19, 10) 
```

**6. 2020-2022**

*This is the remains and is not a decade, but it represents the most contemporary scholarship, so I am considering this set of articles as a piece.*

* Reading the `.txt` files

```{r}
# read all the content from one decade of .txt files into a data frame.
raw_corpus20_22 <- tibble(file = dir("Private/2020-2022", full.names = TRUE))%>%
    mutate(text = map(file, read_lines, skip = 2, skip_empty_rows = TRUE, n_max = Inf,locale = default_locale(), na = character(), num_threads = readr_threads())) %>%
    transmute(id = basename(file), text) %>%
    unnest(text) %>%
#replace punctuation signs and numbers with spaces in column 'text'
    mutate(text = str_replace_all(text, "[[:punct:]]+|[0-9]+", " ")) 
  #replace spaces with underscore and other changes in column id
raw_corpus20_22$id <- stri_replace_all_regex(raw_corpus20_22$id, pattern=c(" ","JLR","[aeiou]"), replacement=c("_","JY_","x"), vectorize=F)

#visualization
sample_n(raw_corpus20_22, 10) 
```

### Turning raw data frames on tidy data frames of tokens

```{r}
# preparing list of the data frames by decade
raw_corpus_all =list(raw_corpus69_79, raw_corpus80_89, raw_corpus90_99, raw_corpus00_09 , raw_corpus10_19, raw_corpus20_22)
#names
names(raw_corpus_all) <- c("tidy_corpus69_79", "tidy_corpus80_89", "tidy_corpus90_99", "tidy_corpus00_09" , "tidy_corpus10_19", "tidy_corpus20_22")
#results
raw_corpus_all %>% str(1)

#Loading custom stopwords
My_stopwords <- read_lines("My_stopwords.csv")

#tokenizing across data frames
tidy_corpus_all <- raw_corpus_all %>% 
  map(~ .x %>% 
    unnest_tokens(word, text))
#Apply stop words
tidy_corpus_all <- tidy_corpus_all %>% 
  map(~ .x %>% 
        anti_join(stop_words %>% 
               filter(lexicon=="snowball") %>% 
               rbind(tibble(lexicon = "custom", word = My_stopwords)))) %>%
  #Creating a column with the year of the data and sorting columns
              map(~ .x %>% 
                    mutate (year = str_extract(id, "\\d+")) %>% 
                    relocate(year, id, word)) 
#final tidy corpus object
tidy_corpus_all %>% str(1)

#Individual tidy data frames saved as Rds. Almost all of them are too big for GitHub, so I only can share the file with less than 25 MB in Data_product_samples
##size
tidy_corpus_all %>%
  map_chr(~ .x %>% 
            object.size() %>% 
            format("Mb")) 
##saving individual data frames as Rds  
if (!dir.exists("Private/tidy_corpus_all")) {
  dir.create("Private/tidy_corpus_all")
}
tidy_corpus_all%>%
  iwalk(~ write_rds(.x, paste0("Private/tidy_corpus_all/", .y, ".Rds")))
```
*Note: as files are heavy, I only could share in GitHub the last one*

* **creating singles data frames for further use**

**Data frame corpus 1969 to 1979**

```{r}
#Data frame
tidy_corpus69_79 <- tidy_corpus_all$tidy_corpus69_79

#Visualization
sample_n(tidy_corpus69_79, 20)
```

**Data frame corpus 1980 to 1989**

```{r}
#Data frame
tidy_corpus80_89 <- tidy_corpus_all$tidy_corpus80_89

#Visualization
sample_n(tidy_corpus80_89, 20)
```
**Data frame corpus 1990 to 1999**

```{r}
#Data frame
tidy_corpus90_99 <- tidy_corpus_all$tidy_corpus90_99

#Visualization
sample_n(tidy_corpus90_99, 100)
```

**Data frame corpus 2000 to 2010**

```{r}
#Data frame
tidy_corpus00_09 <- tidy_corpus_all$tidy_corpus00_09

#Visualization
sample_n(tidy_corpus00_09, 100)
```

**Data frame corpus 2010 to 2019**

```{r}
#Data frame
tidy_corpus10_19 <- tidy_corpus_all$tidy_corpus10_19

#Visualization
sample_n(tidy_corpus10_19, 100)
```

**Data frame corpus 2020 to 2022**

```{r}
#Data frame 
tidy_corpus20_22 <- tidy_corpus_all$tidy_corpus20_22

#Visualization
sample_n(tidy_corpus20_22, 100)
```

---

## Initial processing

---


### General sense of the data by decade

To understand some generalities of the data, I combine the data frames into one master data frame with all the tokens. Then, I manipulated the data to look at trends over time. 

* Master data frame

```{r}
# Combining data frames
master_tidy_corpus <- bind_rows(tidy_corpus69_79, tidy_corpus80_89, tidy_corpus90_99, tidy_corpus00_09 , tidy_corpus10_19, tidy_corpus20_22)

#generalities
## word count
head(master_tidy_corpus %>% 
    count(word, sort = TRUE), 30)
##summary
summary(master_tidy_corpus)  

```

* Plot

```{r}
master_tidy_corpus %>%
  count(word, sort = TRUE)%>%
  filter(n > 30000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

### Description of my data so far

After all the processes to tidy my data, I finally have a data frame useful for applying topic modeling. My master data frame consists of 10,506,625 tokens representing the "cleaning" words from 3,131 articles published between 1969 and 2022 in two literacy education journals&mdash;a leading research journal and a conference papers journal, both from the same disciplinary association. 
The master data frame has 131,611 unique words. The plot above indicates the words repeated over 30,000 times across the data frame. In general, it seems to represent the scope of the focal journals, anchored in literacy education research. More significance will be identified through the topic modeling analysis.  

---


## Topic modeling

For this report, I tried to start testing the codes for topic modeling explained in the book [Text Mining with R](https://www.tidytextmining.com/index.html). However, I had many problems with the codes. As the pre-processing and initial processing of my data was unexpectedly slow, I could not present topic modeling analysis this time.


---

## Next steps

1. Finishing the testing and publishing the general topic modeling analysis.

2. Apply the model to analyze in deep each decade.

3. Compare results of topics between decades with more detail.

4. Create the final report of my results.

5. Discuss the results in relation to my research questions:

* What are the trends in topics of literacy education research and scholarship over more than five decades (1969-2021) of the focal journals?

* How do the topics have changed over time?




```{r}
sessionInfo()
```
