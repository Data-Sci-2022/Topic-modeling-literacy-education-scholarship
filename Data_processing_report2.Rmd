---
title: "Final project; Data processing - Data Science for Linguists (LING 2340)
Fall 2022"
subtitle: "Data Processing - Progress report 2"
author: "Gianina Morales"
date: "11/15/2022"
output:
  github_document:
    toc: TRUE
---
# Data processing - Progress report 2

## Pre-processing: Creating and tidying dataframes from the corpus
```{r setup, message=FALSE, warning=FALSE}
library(tidytext)
library(tidyverse)
library(tm)
library(tidyr)
library(stopwords)
library(stringr)
```

### 1. Turning raw data on data frames

***Note***

*After further discussion with my advisor on the possible results of my project, the R processing of my initial data, and the confidential agreement with the publishers of the "leading journal in the field of Literacy education," I decided to include data from a second journal in my corpus. This time, I am considering the journal that publishes the paper conferences of the annual conference of the same organization in charge of the "leading journal in the field of Literacy education." This movement will have three benefits:* 

* *I can share a sample of my raw data (open-access articles).*
* *I will have a more significant corpus to analyze trends over time in topics and language present in a community of literacy research (the leading association).*
* *I will have more tools to comply with the ethics of avoiding individualization in the data analysis*. 

I divided the data into decades. As I want to compare trends on topics over time, I need to apply the topic modeling method to each decade first and then compare the results. 

*1969-1979*

**Note: I am considering the year in which the journal started to facilitate further comparison with previous content analysis studies.**

*1980-1989*

*1990-1999*

*2000-2009*

*2010-2019*

*2020-2022*

**This is the remains and is not a decade, but it represents the most contemporary scholarship, so I am considering this set of articles as a piece.**

```{r}
# read all the content from one decade of .txt files into a data frame.
raw_corpus10_19 <- tibble(file = dir("Private/2010-2019", full.names = TRUE))%>%
    mutate(text = map(file, read_lines, skip = 1, skip_empty_rows = TRUE, n_max = Inf,locale = default_locale(), na = character(), num_threads = readr_threads(), progress = show_progress())) %>%
    transmute(id = basename(file), text) %>%
    unnest(text) %>%
#replace dots, underscore and numbers with spaces in column 'text'
    mutate(text = str_replace_all(text, "_|\\.|[0-9]+", "")) 
  #replace spaces with underscore and other changes in column id
raw_corpus10_19$id <- gsub(" ", "_", raw_corpus10_19$id) 
raw_corpus10_19$id <- gsub("JLR", "JY_", raw_corpus10_19$id)
raw_corpus10_19$id <- gsub("[aeiou]", "x", raw_corpus10_19$id)
  
head(raw_corpus10_19) 
```




### 2. "Tokenizing" and cleaning the data

As am interested in working by decade, I created an object with the different data frames. Each data frame is tokenized and cleaned to have tidy data. 

```{r}
#Tokens of one word by row
tidy_corpus10.19 <- raw_corpus10_19 %>%
  unnest_tokens(word, text) 
  
head(tidy_corpus10.19)
tidy_corpus10.19

#create a list of stopwords additionally to snowball (base from NLTK's to eliminate "giberish" and similar)
My_stopwords <- c("rf", "ve", "ce", 'el', 'ww', "en", "los", "las", "la", "de", "del", "--", "---", "0o", "0s", "3a", "3b", "3d", "6b", "6o", "a", "a1", "a2", "a3", "a4", "ab", "ag", "aj", "0o", "0s", "3a", "3b", "3d", "6b", "6o", "a", "a1", "a2", "a3", "a4", "ab", "abst", "ac", "ad", "adj", "ae", "af", "ag", "ah", "ain", "aj", "al", "all", "ao", "ap", "ar", "av","aw", "ax", "ay", "az", "b", "b1", "b2", "b3", "ba", "bc", "bd", "be", "bi", "bj", "bk", "bl", "bn", "bp", "br", "bs", "bt", "bu", "bx", "by", "c", "c1", "c2", "c3", "ca", "cc", "cd", "ce", "cf", "cg", "ch", "ci", "cit", "cj", "cl", "cm",  "cn", "co", "com", "con", "cp", "cq", "cr", "cs", "c's", "ct", "cu", "cv", "cx", "cy", "cz", "d", "d2", "da", "dc", "dd", "de", "df", "di", "dj", "dk", "dl", "don", "dp", "dr", "ds", "dt", "du", "dx", "dy", "e", "e2", "e3", "ea", "ec", "ed", "edu", "ee", "ef", "eg", "ei", "ej", "el", "em", "en", "eo", "ep", "eq", "er", "es", "est", "et", "et-al", "etc", "eu", "ev", "ex", "ey", "f", "f2", "fa", "fc", "ff", "fi", "fj", "fl", "fn", "fo", "fr", "fs", "ft", "fu", "fy", "g", "ga", "ge", "gi", "gj", "gl", "go", "gr", "gs", "gy", "h", "h2", "h3", "hh", "hi", "hj", "ho", "hr", "hs", "http", "hu", "hy", "i", "i2", "i3", "i4", "i6", "i7", "i8", "ia", "ib", "ibid", "ic", "id", "i'd", "ie", "if", "ig", "ih", "ii", "ij", "il", "in", "inc", "io", "ip", "iq", "ir", "itd", "iv", "ix", "iy", "iz", "j", "jj", "jr", "js", "jt", "ju", "k", "ke", "kg", "kj", "km", "ko", "l", "l2", "la", "lb", "lc", "le", "les", "lf",  "lj", "ll", "ll", "ln", "lo", "los", "lr", "ls", "lt", "ltd", "m", "m2", "ma", "me", "mg", "ml", "mn", "mo", "mr", "mrs", "ms", "mt", "mu", "n", "n2", "na", "nc", "nd", "ne", "ng", "ni", "nj", "nl", "nn", "no", "nr", "ns", "nt", "ny", "o", "oa", "ob", "oc", "od", "of", "og", "oh", "oi", "oj", "ok", "okay", "ol", "om", "on", "oo", "op", "oq", "or", "ord", "os", "ot", "ou", "ow", "ox", "oz", "p", "p1", "p2", "p3", "pas", "pc", "pd", "pe", "per", "pf", "ph", "pi", "pj", "pk", "pl", "pm", "pn", "po", "pp", "pq", "pr", "ps", "pt", "pu", "put", "py", "q", "qj", "qu", "que", "qv", "r", "r2", "ra", "rc", "rd", "re", "ref", "refs", "rf", "rh", "ri", "rj", "rl", "rm", "rn", "ro", "rq", "rr", "rs", "rt", "ru", "run", "rv", "ry", "s", "s2", "sa", "sc", "sd", "se", "sec", "sf", "si", "sj", "sl", "sm", "sn", "so", "sp", "sq", "sr", "ss", "st", "sub", "sup", "sy", "sz", "t", "t1", "t2", "t3", "tb", "tas", "tc", "td", "te", "tf", "th", "ti", "til", "tip", "tj", "tl", "tm", "tn", "to", "tp", "tq", "tr", "ts", "t's", "tt", "tv", "tx", "u", "u201d", "ue", "ui", "uj", "uk", "um", "un", "unto", "uo",  "ups", "ur", "ut", "v", "va", "vd", "ve", "vj", "vo", "vol", "vols", "vq", "vs", "vt", "vu", "w", "wa", "wi", "wo", "www", "x", "x1", "x2", "x3", "xf", "xi", "xj", "xk", "xl", "xn", "xo", "xs", "xt", "xv", "xx", "y", "y2", "yes", "yet", "yj", "yl", "yr", "ys", "yt", "z", "zi", "zz", "doi", "pre", "rst", "dv", "uqlp")

#Apply stop words
tidy_corpus10.19  <- tidy_corpus10.19 %>%
   anti_join(stop_words %>% 
               filter(lexicon=="snowball") %>% 
               rbind(tibble(lexicon = "custom", word =My_stopwords)))

head (tidy_corpus10.19)
```

## Processing 

### General sense of the data by decade

To understand some generalities of the data, I manipulated the data frames to look at trends over time. With that purpose, I created a unique data frame with all the tokens.




* Word count and plot

```{r}
tidy_corpus10.19 %>%
  count(word, sort = TRUE)%>%
  filter(n > 3000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)

```

*Explanation*

The data frame `tidy_corpus10.19` represents all the words (minus the "stop words") from the articles published by a leading journal in the field of literacy education between the years 2010 and 2019. This data frame is composed by 38,562 unique words. In the chart, it is displayed the words that appears more than 3,000 in the corpus. At first sight it identifies clearly the focus of the journal: literacy research and teaching in school settings. Although interesting, this analysis is not enough to answer my research questions. To advance further, I need to use this data, and the data from the other decades of the journal, to apply a topic modeling methodology. This exercise only represents the first step in the analysis of my data.

2. Creating a column with the year of the data

```{r}
tidy_corpus10.19 <- 
tidy_corpus10.19 %>%
  mutate (year = str_extract(id, "\\d+"))

tidy_corpus10.19

```

3. ***Rds***

I saved the product of tidy tokenization in an Rd file
```{r}
save(tidy_corpus10.19, file = "Private/tidy_corpus10.19.Rds")
```

## Next steps

1. "Troubleshoting" to see how to work with all the data by once and not by decade-group

2. Run topic modeling coding with the data. 

3. Test parameters to prepare the final model for analysis.


```{r}
sessionInfo()
```
